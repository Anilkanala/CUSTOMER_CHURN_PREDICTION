# -*- coding: utf-8 -*-
"""Project (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BovM_WGIU4z4MPa-awanKvZDjciIEZ6y
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("customer_churn_prediction.csv")

df.head()

df.tail()

df.describe()

df.info()

df.isnull().mean()*100

# Check the number of missing values per column
missing_values = df.isnull().sum()

# Drop rows with missing values since it's just one row missing per column
df_cleaned = df.dropna()

# Confirm no missing values remain
missing_values_after = df_cleaned.isnull().sum()

# Summary statistics for numerical columns
summary_stats = df_cleaned.describe()

missing_values, missing_values_after, summary_stats

numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns

num_cols = len(numerical_cols)
rows = (num_cols // 3) + 1  # Adjust rows dynamically
cols = 3  # Display 3 plots per row

# Create figure
fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))
axes = axes.flatten()  # Flatten to handle indexing easily

# Loop through numerical columns and create boxplots
for i, col in enumerate(numerical_cols):
    sns.boxplot(x=df[col], ax=axes[i])
    axes[i].set_title(f'Boxplot of {col}')

# Remove empty subplots if any
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()  # Adjust layout for better spacing
plt.show()

# Plot categorical variables vs Churn
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Gender vs Churn
sns.countplot(x="Gender", hue="Churn", data=df_cleaned, palette="coolwarm", ax=axes[0])
axes[0].set_title("Gender vs Churn")

# Subscription Type vs Churn
sns.countplot(x="Subscription Type", hue="Churn", data=df_cleaned, palette="coolwarm", ax=axes[1])
axes[1].set_title("Subscription Type vs Churn")

# Contract Length vs Churn
sns.countplot(x="Contract Length", hue="Churn", data=df_cleaned, palette="coolwarm", ax=axes[2])
axes[2].set_title("Contract Length vs Churn")

plt.tight_layout()
plt.show()

# Convert categorical variables into numerical format using label encoding
from sklearn.preprocessing import LabelEncoder

# Identify categorical columns
categorical_cols = ["Gender", "Subscription Type", "Contract Length"]

# Apply Label Encoding
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    df_cleaned[col] = le.fit_transform(df_cleaned[col])
    label_encoders[col] = le  # Store encoders for future reference

# Reattempt the correlation heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(df_cleaned.corr(numeric_only=True), annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Feature Correlation Heatmap (Fixed)")
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Define features (X) and target variable (y)
X = df_cleaned.drop(columns=["CustomerID", "Churn"])  # Drop ID column
y = df_cleaned["Churn"]

# Split into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Standardize numerical features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Confirm shape of training and testing data
X_train_scaled.shape, X_test_scaled.shape, y_train.shape, y_test.shape

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

# Train Logistic Regression model
model = LogisticRegression(random_state=42)
model.fit(X_train_scaled, y_train)

# Make predictions on test data
y_pred = model.predict(X_test_scaled)

# Evaluate model performance
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

accuracy, precision, recall, f1, report

from sklearn.ensemble import RandomForestClassifier

# Train a Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_scaled, y_train)

# Make predictions
y_pred_rf = rf_model.predict(X_test_scaled)

# Evaluate performance
accuracy_rf = accuracy_score(y_test, y_pred_rf)
precision_rf = precision_score(y_test, y_pred_rf)
recall_rf = recall_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)
report_rf = classification_report(y_test, y_pred_rf)

accuracy_rf, precision_rf, recall_rf, f1_rf, report_rf

importances = rf_model.feature_importances_
feature_names = X.columns

# Create a DataFrame for feature importance
feat_importance_df = pd.DataFrame({"Feature": feature_names, "Importance": importances})
feat_importance_df = feat_importance_df.sort_values(by="Importance", ascending=False)

# Plot feature importance
plt.figure(figsize=(10, 6))
sns.barplot(x="Importance", y="Feature", data=feat_importance_df, palette="viridis")
plt.title("Feature Importance - Random Forest Model")
plt.xlabel("Importance Score")
plt.ylabel("Feature")
plt.show()